{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe5e80f2-bb95-43ce-8408-8f20afea9ab2",
   "metadata": {},
   "source": [
    "## backPropogation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb5d4bb7-88bb-437e-8189-e3cbfc7981ef",
   "metadata": {},
   "source": [
    "// it is a inverse of forward propogation,with help of backpropogation we can update value of weight and bias and reduce loss(error)\n",
    "and gradient decent is used in backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b9698-b913-41f0-98c1-eaa87f42fcb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4de4f4-0cc4-4fc1-ba56-8239d3c4241b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2541f50-1c43-4ffd-ba5c-54fa994cf80b",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5bc273c5-7c05-4975-84e5-d02cd63beb7d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Types of activation function\n",
    "1 - Binary step Function   // if x>0 then 1 else 0 \n",
    "2 - Linear activation function // f(x) = x\n",
    "\n",
    "3 - Non linear activation function // generalizing and adapting any sort od data in order to perform correct differentiation among the output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ed4b126-a9ef-45e5-b4d9-98bf4ff76802",
   "metadata": {},
   "source": [
    "// Activation function should be differentiable so that it usefu in gradiant decent\n",
    "** Sigmoid\n",
    "/ sigmoid is differentiable but not zero centered\n",
    "/ desired output is in binary  **0 or 1\n",
    "# Types of non linear activation function\n",
    "\n",
    "1 - Sigmoid/Logistic activation function\n",
    "2 - Tannh Function(Hyperbolic Tangent)\n",
    "3 - ReLU Function\n",
    "4 - Softmax Function"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b40e2480-8c71-45f9-83ba-e578ca276b86",
   "metadata": {},
   "source": [
    "** Tanh Function// it is used when we work on RNN(recurrent neural network) because its result is between -1 to 1\n",
    "/ it is differentiable and zero centered(function mean is zero) also\n",
    "/ desired output is in binary"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07f4fc01-f650-41b6-b8c5-673fa2ddeaca",
   "metadata": {},
   "source": [
    "** Softmax Function\n",
    "/ desired output more then two(classification)\n",
    "/ it is differentiable"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e569fe6c-0239-492b-96c0-52c76b7d6ffc",
   "metadata": {},
   "source": [
    "** ReLU FUnction\n",
    "/ used in hidden layer of neural network"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb2e2cb7-d121-46af-aaa3-63e2ba765d40",
   "metadata": {},
   "source": [
    "// choose activation function\n",
    "Regression - Linear Activation Function\n",
    "Binary Classification - Sigmoid\n",
    "Multiclass classification - Softmax\n",
    "Multilable classification - sigmoid\n",
    "convolution neyral network - ReLU\n",
    "Hidden layer - ReLU\n",
    "recurrent NN - sigmoid / Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4226c3ed-418e-4be5-8114-97ebbb46368b",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "raw",
   "id": "06712500-82d5-49f2-ad61-b57ec03f83ef",
   "metadata": {},
   "source": [
    "// Loss function is difference between original output and predicted output on single data point\n",
    "// and finding loss on entire data and take mean of it is called Cost Function"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed1d0d83-05b4-4b70-b192-23be78ac358d",
   "metadata": {},
   "source": [
    "/ loss function evalute how well model work and error"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5af379c-732b-4c1a-b9bf-c31be61a892e",
   "metadata": {},
   "source": [
    "Types of Loss Function\n",
    "1 - Regression\n",
    "    mean square error  // mean of square of difference between original value - predicted value\n",
    "    absolute mean error // mean of mode of difference between original value - predicted value\n",
    "                        // it is not differentiable\n",
    "    hubber error  // work on hyper parameter, if greater than hyper parameter work as absolute mean error else like mean square error\n",
    "                  // when in dataset outlier is 30% or more than it\n",
    "2 - classification\n",
    "    binary cross entropy/log loss  //used when result od data is in binary form\n",
    "    categorial cross entropy   //used when data is in catogarical form more then two results,here it convert it into one hot code\n",
    "                               //when classification is more then use sparse catogarical cross entropy,here it convert it into label data \n",
    "3 - Auto encoder\n",
    "    KL Divergence\n",
    "4 - Gan\n",
    "    Discriminator loss\n",
    "    Minmax gan loss\n",
    "5 - Object detection\n",
    "    Focal loss\n",
    "6 - Word embedding\n",
    "    triplet loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a49e6-d7ca-4f04-9e08-8f6efac60b35",
   "metadata": {},
   "source": [
    "# optimizer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "397fee03-ed92-43a5-b19a-305ec427566a",
   "metadata": {},
   "source": [
    "optimizers are algorithms or methods to used to change the attributes of your neural networks such as weight and learning rate in order to reduce the losses."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cfe7ac0-1ab4-4753-b3e0-627d41dd01d4",
   "metadata": {},
   "source": [
    "#Types of optimizer\n",
    "1 - Gradient descent\n",
    "2 - Stochatic gradient descent\n",
    "3 - Stochatic gradient descent with momentum\n",
    "4 - mini batch gradient descent\n",
    "5 - Adagrad\n",
    "6 - RMSProp\n",
    "7 - AdaDelta\n",
    "8 - Adam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
